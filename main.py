# Prompts:

# Act as an AI programmer specializing in audio processing with music talent.

# GOAL1: Objective:
# 1. Create a synthesizer that plays sounds like water droplets falling sound.

# GOAL2: Description:
# The goal of this project is to create a synthesizer that can play sounds like water droplets falling sound from Sinewave
# The synthesizer will be designed to receive MIDI input and output audio signals accordingly.

# GOAL3: Features:
# 1. The synthesizer will generate sound effects that simulate water droplets falling sound.
# 2. Import from .mid file and convert the signals to generate corresponding sound effects, export to .wav
# 3. The synthesizer is monophonic.
# 4. The sound generated by the synthesizer will be adjustable in terms of volume, tone, and other parameters to create a unique sound.

# GOAL4: Technical specifications:
# 1. The synthesizer will be designed using digital signal processing techniques.
# 2. Use Python.
# 3. The sound effects will be generated using mathematical algorithms from Sinewave to simulate the sound of water droplets falling.
# 4. The sound effects uses CombFilter and Reverb.

# GOAL5: File structure:
# 1. Write main file named main.py
# 2. midifile named input.mid

import numpy as np
from scipy.io.wavfile import write
import mido
import math
from scipy.signal import butter, lfilter, resample

def normalize_audio(audio_data):
    # Calculate the maximum absolute value in the audio_data
    max_abs_value = np.abs(audio_data).max()

    # Normalize the audio_data by dividing each element by the max_abs_value
    normalized_audio = audio_data / max_abs_value

    # Return the normalized audio data
    return normalized_audio

def butter_lowpass(cutoff, sample_rate, order=5):
    nyq = 0.5 * sample_rate
    normal_cutoff = cutoff / nyq
    b, a = butter(order, normal_cutoff, btype='low', analog=False)
    return b, a

def lowpass_filter(data, cutoff, sample_rate, order=5):
    b, a = butter_lowpass(cutoff, sample_rate, order=order)
    return lfilter(b, a, data)

def comb_filter(signal, delay, gain):
    output = np.zeros_like(signal)
    buffer = np.zeros(delay)
    index = 0
    for i, x in enumerate(signal):
        buffer_value = buffer[index]
        output[i] = x + gain * buffer_value
        buffer[index] = output[i]
        index = (index + 1) % delay
    return output

def ping_pong_delay(signal, delay_left, delay_right, gain):
    output = np.zeros_like(signal)
    buffer_left = np.zeros(delay_left)
    buffer_right = np.zeros(delay_right)
    index_left = 0
    index_right = 0

    for i, x in enumerate(signal):
        buffer_value_left = buffer_left[index_left]
        buffer_value_right = buffer_right[index_right]

        if i % 2 == 0:
            output[i] = x + gain * buffer_value_left
            buffer_right[index_right] = output[i]
            index_left = (index_left + 1) % delay_left
        else:
            output[i] = x + gain * buffer_value_right
            buffer_left[index_left] = output[i]
            index_right = (index_right + 1) % delay_right

    return output

def noisy_reverb(signal, delay, gain, noise_factor):
    output = np.zeros_like(signal)
    buffer = np.zeros(delay)
    index = 0
    
    for i, x in enumerate(signal):
        buffer_value = buffer[index]
        noise = noise_factor * np.random.normal(0, 1)
        output[i] = x + gain * (buffer_value + noise)
        buffer[index] = output[i]      
        index = (index + 1) % delay
        
    return output

def water_ripple(signal, freqs, sample_rate, gain=1.0):
    t = np.arange(len(signal)) / sample_rate
    water_rippled_signal = signal.astype(np.float32)  # Convert signal to float32

    for freq in freqs:
        modulating_wave = (1 + np.sin(2 * np.pi * freq * t)).astype(np.float32)  # Convert modulating_wave to float32
        water_rippled_signal *= modulating_wave

    # Apply gain to control the volume
    water_rippled_signal *= gain

    return water_rippled_signal
    
def generate_water_droplet_sound(freq, duration, sample_rate):
    n_samples = max(int(duration * sample_rate), 1)
    t = np.arange(0, n_samples) / sample_rate
    
    droplet_base = np.sin(2 * np.pi * freq * t)

    # FM synthesis for glass-like transparency
    modulating_freq = freq * 9
    mod_index = 11
    modulating_wave = np.sin(2 * np.pi * modulating_freq * t)

    fm_transparent = np.sin(2 * np.pi * (freq + mod_index * modulating_wave) * t)

    # Pad fm_transparent with zeros
    if len(fm_transparent) < len(droplet_base):
        padding = np.zeros(len(droplet_base) - len(fm_transparent))
        fm_transparent = np.concatenate((fm_transparent, padding))

    # Add granular delay
    def apply_granular_delay(signal, grain_size, overlap_factor, window_function):
        hop_size = int(grain_size / overlap_factor)
        n_hops = (len(signal) - grain_size) // hop_size
        output_signal = np.zeros(n_hops * hop_size + grain_size)

        grains = [signal[i * hop_size:i * hop_size + grain_size] * window_function for i in range(n_hops)]

        for i, grain in enumerate(grains):
            output_signal[i * hop_size:i * hop_size + grain_size] += grain

        return output_signal
    
    # Add granular delay
    grain_size = int(sample_rate / (freq * 4))
    overlap_factor = 16
    window_function = np.hanning(grain_size)
    granular_delayed_sound = apply_granular_delay(fm_transparent, grain_size, overlap_factor, window_function)

    # Pad granular_delayed_sound with zeros
    if len(granular_delayed_sound) < len(droplet_base):
        padding = np.zeros(len(droplet_base) - len(granular_delayed_sound))
        granular_delayed_sound = np.concatenate((granular_delayed_sound, padding))


    # Combine the droplet base, FM transparent sound, and granular delay
    combined_sound = droplet_base + 0.25 * fm_transparent + 0.25 * granular_delayed_sound

    # Use the same envelope from the previous version
    attack_time = 0.0005
    release_time = 3
    envelope = (1 - np.exp(-t / attack_time)) * np.exp(-t / release_time)

    sound = combined_sound * envelope

    return sound

def read_midi_file(file_path):
    mid = mido.MidiFile(file_path)

    note_events = []
    tempo = None
    note_start_times = {}
    accumulated_time = 0

    default_tempo = 120

    for msg in mid:
        # Set the tempo if not already set.
        if msg.type == 'set_tempo':
            tempo = msg.tempo

        # Update the accumulated time.
        accumulated_time += mido.tick2second(msg.time, mid.ticks_per_beat, tempo if tempo is not None else default_tempo)

        # Process 'note_on' and 'note_off' events per channel.
        if msg.type in ('note_on', 'note_off'):
            # Only consider 'note_on' events with velocity > 0, or 'note_off' events.
            if (msg.type == 'note_on' and msg.velocity > 0) or (msg.type == 'note_off') or (msg.type == 'note_on' and msg.velocity == 0):
                channel = msg.channel

                # Check if a note is starting or ending.
                if channel not in note_start_times:
                    note_start_times[channel] = {}

                if msg.type == 'note_on' and msg.velocity > 0:
                    note_start_times[channel][msg.note] = accumulated_time
                elif (msg.type == 'note_off' or (msg.type == 'note_on' and msg.velocity == 0)) and msg.note in note_start_times[channel]:
                    start_time = note_start_times[channel][msg.note]
                    duration = accumulated_time - start_time
                    del note_start_times[channel][msg.note]

                    note_freq = 440 * pow(2, (msg.note - 69) / 12)
                    note_events.append((note_freq, duration))  # Remove mido.tick2second()

    note_frequencies, note_durations = zip(*note_events)
    print(f"Note Frequencies: {note_frequencies}")
    print(f"Note Durations: {note_durations}")
    return note_frequencies, note_durations

def synthesizer(note_frequencies, note_durations, sample_rate):
    output_audio = np.zeros(0, dtype=np.int16)
    total_notes = len(note_frequencies)
    min_duration = 1 / sample_rate

    for index, (freq, duration) in enumerate(zip(note_frequencies, note_durations)):
        # Ensure note durations are greater than or equal to the minimum duration
        actual_duration = max(duration, min_duration)
        water_droplet_sound = generate_water_droplet_sound(freq, actual_duration, sample_rate)
        print(f"Min-Max value of water_droplet_sound: {water_droplet_sound.min()}, {water_droplet_sound.max()}")

        if index % 2 == 0:
            stereo_audio = np.column_stack((water_droplet_sound.astype(np.float32), np.zeros_like(water_droplet_sound).astype(np.float32)))
        else:
            stereo_audio = np.column_stack((np.zeros_like(water_droplet_sound).astype(np.float32), water_droplet_sound.astype(np.float32)))

        if index == 0:
            output_audio = stereo_audio
        else:
            output_audio = np.vstack((output_audio, stereo_audio))

        output_audio = np.vstack((output_audio, stereo_audio))

        # Progress reporting
        progress = (index + 1) / total_notes * 100
        print(f"Progress: {progress:.2f}%")

    left_channel = output_audio[:, 0]
    right_channel = output_audio[:, 1]

    left_channel = water_ripple(left_channel, freqs=[1, 0.25, 3], sample_rate=44100).astype(np.int16)
    right_channel = water_ripple(right_channel, freqs=[1, 0.6, 2], sample_rate=44100).astype(np.int16)

    output_audio = np.column_stack((left_channel, right_channel))

    return output_audio

def main():
    midi_file_path = 'input.mid'

    # Read the MIDI file
    print("Reading MIDI file...")
    note_frequencies, note_durations = read_midi_file(midi_file_path)

    if not note_frequencies or not note_durations:
        print("No notes found in the MIDI file.")
        return

    # Print information about the notes
    print(f"Total Notes: {len(note_frequencies)}")
    print(f"Note Frequencies: {note_frequencies}")
    print(f"Note Durations: {note_durations}")

    # Synthesize water droplet sounds
    print("Starting synthesis...")
    note_durations = [duration * 1000 for duration in note_durations]
    output_audio = synthesizer(note_frequencies, note_durations, sample_rate=44100)
    # Normalize the output audio
    max_value_left = np.abs(output_audio[:, 0]).max()
    max_value_right = np.abs(output_audio[:, 1]).max()
    output_audio[:, 0] = ((output_audio[:, 0] / max_value_left) * (2**15 - 1)).astype(np.int16)
    output_audio[:, 1] = ((output_audio[:, 1] / max_value_right) * (2**15 - 1)).astype(np.int16)
    
    # Check the output audio array
    print(f"Output Audio Shape: {output_audio.shape}")
    print(f"Output Audio Data: {output_audio}")
    print(f"Min-Max value of output_audio (left channel): {output_audio[:, 0].min()}, {output_audio[:, 0].max()}")
    print(f"Min-Max value of output_audio (right channel): {output_audio[:, 1].min()}, {output_audio[:, 1].max()}")

    # Save the output as .wav file
    output_file_path = 'output_water_droplets.wav'
    write(output_file_path, 44100, output_audio)

if __name__ == "__main__":
    main()